{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:27.550084Z",
     "start_time": "2024-12-02T09:45:27.368031Z"
    }
   },
   "source": [
    "from typing import Final\n",
    "\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:27.565507Z",
     "start_time": "2024-12-02T09:45:27.557506Z"
    }
   },
   "source": [
    "RANDOM_SEED:  Final[int]    = 42\n",
    "TEST_SIZE:    Final[float]  = 0.2\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:27.676294Z",
     "start_time": "2024-12-02T09:45:27.663837Z"
    }
   },
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Berechnet die Entropie eines gegebenen Datensatzes y.\n",
    "\n",
    "    Args:\n",
    "        y (array-like): Ein Array oder eine Liste von Zielwerten (Labels)\n",
    "\n",
    "    Returns:\n",
    "        float: Die Entropie des Datensatzes y.\n",
    "\n",
    "    Beispiel:\n",
    "        entropy([1, 1, 0, 0]) → 1.0\n",
    "    \"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    prob = counts / len(y)\n",
    "\n",
    "    # y = H(V) = -Σ P(Bj) * log2(P(Bj))\n",
    "    return -np.sum(prob * np.log2(prob))\n",
    "\n",
    "\n",
    "def information_gain(X, y, feature_index):\n",
    "    \"\"\"\n",
    "    Berechnet den Informationsgewinn für ein bestimmtes Feature im Datensatz.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Das Array der Eingabefeatures\n",
    "        y (array-like): Die Labels, die mit X verknüpft sind.\n",
    "        feature_index (int): Der Index des Features, für das der Informationsgewinn berechnet wird.\n",
    "\n",
    "    Returns:\n",
    "        float: Der Informationsgewinn für das angegebene Feature.\n",
    "\n",
    "    Beispiel:\n",
    "        information_gain(X, y, 0) → 0.5\n",
    "    \"\"\"\n",
    "    parent_entropy = entropy(y)\n",
    "    unique_values = np.unique(X[:, feature_index])\n",
    "\n",
    "    weighted_entropy = 0\n",
    "    for value in unique_values:\n",
    "        subset_y = y[X[:, feature_index] == value]\n",
    "\n",
    "        # P(Ai | Bj) * H(V | Bj)\n",
    "        weighted_entropy += (len(subset_y) / len(y)) * entropy(subset_y)\n",
    "\n",
    "    # V = Zielvariable, W = Feature\n",
    "    # IG(V, W) = H(V) - H(V|W)\n",
    "    return parent_entropy - weighted_entropy\n",
    "\n",
    "\n",
    "def best_split(X, y):\n",
    "    \"\"\"\n",
    "    Bestimmt das beste Feature für den Split, basierend auf dem maximalen Informationsgewinn.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Das Array der Eingabefeatures.\n",
    "        y (array-like): Das Ziel-Array (Labels), das mit X verknüpft ist.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Ein Tupel, das den Index des besten Features und den maximalen Informationsgewinn enthält.\n",
    "\n",
    "    Beispiel:\n",
    "        best_split(X, y) → (0, 0.25)\n",
    "    \"\"\"\n",
    "    best_feature_index = None\n",
    "    best_info_gain     = -1\n",
    "\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        info_gain = information_gain(X, y, feature_index)\n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain     = info_gain\n",
    "            best_feature_index = feature_index\n",
    "\n",
    "    return best_feature_index, best_info_gain"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:27.722214Z",
     "start_time": "2024-12-02T09:45:27.695986Z"
    }
   },
   "source": [
    "class MyDecisionTree:\n",
    "    \"\"\"\n",
    "    Ein einfacher Entscheidungsbaum-Algorithmus für binäre Klassifikation.\n",
    "\n",
    "    Args:\n",
    "        max_depth (int): Die maximale Tiefe des Entscheidungsbaums. Wenn nicht angegeben, wird der Baum so tief wie nötig gebaut.\n",
    "\n",
    "    Methoden:\n",
    "        - fit(X, y): Trainiert den Entscheidungsbaum mit den Eingabedaten X und den Zielwerten y.\n",
    "        - predict(X): Gibt die vorhergesagten Labels für die Eingabedaten X zurück.\n",
    "\n",
    "    Beispiel:\n",
    "        tree = MyDecisionTree(max_depth=3)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "    \n",
    "    Referenzen:\n",
    "        - Vorlesungsfolien: ML_VL_9_Entscheidungsbaeume.pdf \n",
    "        - https://www.analyticsvidhya.com/blog/2020/10/all-about-decision-tree-from-scratch-with-python-implementation/\n",
    "        - https://medium.com/@omidsaghatchian/decision-tree-implementation-from-scratch-visualization-5eb0bbf427c2\n",
    "        - https://www.kaggle.com/code/fareselmenshawii/decision-tree-from-scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # Wenn alle Beispiele in der gleichen Klasse sind, ist dies ein Blatt\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return {\"label\": np.unique(y)[0]}\n",
    "\n",
    "        # Wenn keine weiteren Splits möglich sind oder die max. Tiefe erreicht ist, ist dies ein Blatt\n",
    "        if len(X) == 0 or (self.max_depth and depth >= self.max_depth):\n",
    "            return {\"label\": np.bincount(y).argmax()}\n",
    "\n",
    "        feature_index, info_gain = best_split(X, y)\n",
    "\n",
    "        # Wenn der Information Gain 0 ist, ist dies auch ein Blatt (Nicht mehr sinnvoll zu splitten)\n",
    "        if info_gain == 0:\n",
    "            return {\"label\": np.bincount(y).argmax()}\n",
    "\n",
    "\n",
    "        unique_values = np.unique(X[:, feature_index])\n",
    "        tree = {\"feature_index\": feature_index, \"branches\": {}}\n",
    "\n",
    "        for value in unique_values:\n",
    "            subset_X = X[X[:, feature_index] == value]\n",
    "            subset_y = y[X[:, feature_index] == value]\n",
    "            tree[\"branches\"][value] = self._build_tree(subset_X, subset_y, depth + 1)\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_one(self, x, tree):\n",
    "        if \"label\" in tree:\n",
    "            return tree[\"label\"]\n",
    "\n",
    "        feature_value = x[tree[\"feature_index\"]]\n",
    "        branch = tree[\"branches\"].get(feature_value)\n",
    "\n",
    "        if branch is None:\n",
    "            global y_train\n",
    "            return np.bincount(y_train).argmax()\n",
    "\n",
    "        return self._predict_one(x, branch)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:27.768164Z",
     "start_time": "2024-12-02T09:45:27.745749Z"
    }
   },
   "source": [
    "class MyRandomForest:\n",
    "    \"\"\"\n",
    "    Ein einfacher Random Forest-Algorithmus für binäre Klassifikation.\n",
    "\n",
    "    Args:\n",
    "        n_estimators (int): Die Anzahl der Bäume.\n",
    "        max_depth (int): Die maximale Tiefe eines jeden Entscheidungsbaums.\n",
    "        sample_size (float): Der Prozentsatz der Trainingsdaten, die für jeden Baum verwendet werden.\n",
    "\n",
    "    Methoden:\n",
    "        - fit(X, y): Trainiert den RandomForest mit den Eingabedaten X und den Labels y.\n",
    "        - predict(X): Gibt die vorhergesagten Labels für die Eingabedaten X zurück.\n",
    "\n",
    "    Beispiel:\n",
    "        forest = MyRandomForest(n_estimators=100, max_depth=5, sample_size=0.5)\n",
    "        forest.fit(X_train, y_train)\n",
    "        y_pred = forest.predict(X_test)\n",
    "\n",
    "    Referenzen:\n",
    "        - Vorlesungsfolien: ML_VL_9_Entscheidungsbaeume.pdf & ML_VL_10_EnsembleLearning.pdf\n",
    "        - https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249\n",
    "        - https://www.kaggle.com/code/fareselmenshawii/random-forest-from-scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=10, max_depth=None, sample_size=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = int(len(X) * self.sample_size)\n",
    "        indices = np.random.choice(len(X), size=n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = MyDecisionTree(max_depth=self.max_depth)\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        majority_votes = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_predictions)\n",
    "        return majority_votes\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:27.799659Z",
     "start_time": "2024-12-02T09:45:27.791284Z"
    }
   },
   "source": [
    "def my_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    test_size = int(n_samples * test_size)\n",
    "    indices   = np.random.permutation(n_samples)\n",
    "\n",
    "    test_indices  = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def my_accuracy_score(y_true, y_pred):\n",
    "    correct_predictions = np.sum(np.equal(y_true, y_pred))\n",
    "    total_predictions   = len(y_true)\n",
    "\n",
    "    return correct_predictions / total_predictions"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:27.846179Z",
     "start_time": "2024-12-02T09:45:27.832464Z"
    }
   },
   "source": [
    "def my_classification_report(y_true, y_pred):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    report = {}\n",
    "\n",
    "    for label in unique_labels:\n",
    "        true_positive = np.sum(np.logical_and(y_true == label, y_pred == label))\n",
    "        false_positive = np.sum(np.logical_and(y_true != label, y_pred == label))\n",
    "        false_negative = np.sum(np.logical_and(y_true == label, y_pred != label))\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        report[label] = {\"precision\": precision, \"recall\": recall, \"f1-score\": f1_score}\n",
    "\n",
    "    return report\n",
    "\n",
    "def report_to_string(report: dict):\n",
    "    if 'precision' not in report[next(iter(report))] or 'recall' not in report[next(iter(report))] or 'f1-score' not in report[next(iter(report))]:\n",
    "        return \"Precision, Recall and F1-Score are required in the report.\"\n",
    "\n",
    "    report_str = \"\"\n",
    "    for label, metrics in report.items():\n",
    "        report_str += f\"Label {label}:\\n\"\n",
    "        report_str += f\"  Precision: {metrics['precision']:.2f}\\n\"\n",
    "        report_str += f\"  Recall:    {metrics['recall']:.2f}\\n\"\n",
    "        report_str += f\"  F1-Score:  {metrics['f1-score']:.2f}\\n\"\n",
    "    return report_str"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:29.729068Z",
     "start_time": "2024-12-02T09:45:27.909534Z"
    }
   },
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y =  datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "X_numpy = X.to_numpy()\n",
    "y_numpy = y.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = my_train_test_split(X_numpy, y_numpy, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:29.791182Z",
     "start_time": "2024-12-02T09:45:29.763072Z"
    }
   },
   "source": [
    "X.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:29.886245Z",
     "start_time": "2024-12-02T09:45:29.841114Z"
    }
   },
   "source": [
    "forest = MyRandomForest(n_estimators=5, max_depth=3, sample_size=0.8)\n",
    "forest.fit(X_train, y_train)\n",
    "forest_predictions = forest.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:29.964617Z",
     "start_time": "2024-12-02T09:45:29.952516Z"
    }
   },
   "source": [
    "report = my_classification_report(y_test, forest_predictions)\n",
    "print(report_to_string(report))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0:\n",
      "  Precision: 1.00\n",
      "  Recall:    1.00\n",
      "  F1-Score:  1.00\n",
      "Label 1:\n",
      "  Precision: 0.56\n",
      "  Recall:    1.00\n",
      "  F1-Score:  0.72\n",
      "Label 2:\n",
      "  Precision: 1.00\n",
      "  Recall:    0.36\n",
      "  F1-Score:  0.53\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:45:30.433915Z",
     "start_time": "2024-12-02T09:45:30.075370Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sklearn_forest = RandomForestClassifier(n_estimators=5, max_depth=3, random_state=RANDOM_SEED)\n",
    "sklearn_forest.fit(X_train, y_train)\n",
    "sklearn_predictions = sklearn_forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, sklearn_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.90      1.00      0.95         9\n",
      "           2       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-kurs-ws24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
